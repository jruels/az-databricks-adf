{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az group create --name rg-adf-james --location eastus --subscription 33266a41-134d-4de7-a780-665b38b0f7b8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az datafactory create \\\n",
    "    --resource-group rg-adf-james \\\n",
    "    --factory-name adf-james \\\n",
    "    --location eastus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az datafactory list --resource-group rg-adf-james"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a storage account\n",
    "!az storage account create \\\n",
    "    --resource-group rg-adf-james \\\n",
    "    --name jamesaccount2025 \\\n",
    "    --location eastus \\\n",
    "    --sku Standard_LRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a container\n",
    "!az storage container create \\\n",
    "    --account-name jamesaccount2025 \\\n",
    "    --name inputcontainer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload lab2data.csv\n",
    "!az storage blob upload \\\n",
    "    --account-name jamesaccount2025 \\\n",
    "    --container-name inputcontainer \\\n",
    "    --name lab2data.csv \\\n",
    "    --file lab2data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Azure Database for PostgreSQL flexible server\n",
    "!az postgres flexible-server create --subscription 33266a41-134d-4de7-a780-665b38b0f7b8 --resource-group rg-adf-james --name adfpostgres --location canadacentral --version 16 --zone 1 --password-auth enabled --admin-user adfadmin --admin-password adf@min2025 --tier GeneralPurpose --sku-name Standard_D2s_v3 --storage-type premium_lrs --storage-size 128 --performance-tier P10 --storage-auto-grow enabled --high-availability disabled --public-access all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az postgres flexible-server firewall-rule create \\\n",
    "    --resource-group rg-adf-james \\\n",
    "    --name adfpostgres \\\n",
    "    --rule-name AllowAll \\\n",
    "    --start-ip-address 0.0.0.0 \\\n",
    "    --end-ip-address 255.255.255.255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install postgresql if you haven't already\n",
    "!choco install -y postgresql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!psql \"sslmode=require host=adfpostgres.postgres.database.azure.com user=adfadmin dbname=postgres\" -c \"CREATE TABLE public.sample_data (id SERIAL PRIMARY KEY, name VARCHAR(100), value INT);\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!psql \"sslmode=require host=adfpostgres.postgres.database.azure.com user=adfadmin dbname=postgres\" -c \"INSERT INTO public.sample_data(name, value) VALUES ('Test1',123),('Test2',456);\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the data\n",
    "!psql \"sslmode=require host=adfpostgres.postgres.database.azure.com user=adfadmin password=adf@min2025 dbname=postgres\" -c \"SELECT * FROM public.sample_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install azure-mgmt-datafactory azure-identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.identity import AzureCliCredential\n",
    "from azure.mgmt.datafactory import DataFactoryManagementClient\n",
    "from azure.mgmt.datafactory.models import (\n",
    "    LinkedServiceResource,\n",
    "    AzureStorageLinkedService,\n",
    "    AzurePostgreSqlLinkedService,\n",
    "    DatasetResource,\n",
    "    DelimitedTextDataset,\n",
    "    PostgreSqlTableDataset,\n",
    "    CopyActivity,\n",
    "    PipelineResource,\n",
    "    DatasetStorageFormat,\n",
    "    DatasetCompression,\n",
    "    LinkedService,\n",
    "    Dataset,\n",
    "    Activity\n",
    "    )\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Configure Credentials and Clients\n",
    "credential = AzureCliCredential()\n",
    "\n",
    "# Replace with your actual values\n",
    "subscription_id = \"33266a41-134d-4de7-a780-665b38b0f7b8\"\n",
    "resource_group = \"rg-adf-james\"\n",
    "factory_name = \"adf-james\"\n",
    "storage_account_name = \"jamesaccount2025\"\n",
    "postgres_server_name= \"adfpostgres.postgres.database.azure.com\"\n",
    "postgres_admin_user = \"adfadmin\"\n",
    "postgres_admin_password = \"adf@min2025\"\n",
    "storage_container_name = \"inputcontainer\"\n",
    "storage_file_name = \"lab2data.csv\"\n",
    "table_name = \"public.sample_data\"\n",
    "location = \"eastus\"\n",
    "\n",
    "\n",
    "\n",
    "df_client = DataFactoryManagementClient(credential, subscription_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Create Azure Storage Linked Service\n",
    "storage_linked_service = LinkedServiceResource(\n",
    "    properties=AzureStorageLinkedService(\n",
    "    connection_string={\n",
    "        \"type\": \"SecureString\",\n",
    "        \"value\": f\"DefaultEndpointsProtocol=https;AccountName={storage_account_name};EndpointSuffix=core.windows.net;\"\n",
    "    }\n",
    "    ))\n",
    "print(\"Creating storage linked service...\")\n",
    "ls_storage = df_client.linked_services.create_or_update(resource_group,factory_name, \"AzureStorageLinkedService\", storage_linked_service)\n",
    "\n",
    "print(f\"storage linked service created {ls_storage.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Create Azure PostgreSQL Linked Service\n",
    "postgres_linked_service = LinkedServiceResource(\n",
    "    properties=AzurePostgreSqlLinkedService(\n",
    "        server = postgres_server_name,\n",
    "        database = \"postgres\",\n",
    "        username = postgres_admin_user,\n",
    "        password = {\n",
    "            \"type\": \"SecureString\",\n",
    "            \"value\": postgres_admin_password\n",
    "        }\n",
    "    ))\n",
    "print(\"Creating postgres linked service...\")\n",
    "ls_postgres = df_client.linked_services.create_or_update(resource_group, factory_name, \"AzurePostgreSqlLinkedService\", postgres_linked_service)\n",
    "print(f\"postgres linked service created {ls_postgres.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Create Azure Storage CSV Dataset\n",
    "storage_dataset = DatasetResource(\n",
    "        properties=DelimitedTextDataset(\n",
    "        linked_service_name=LinkedService(\n",
    "            reference_name=\"AzureStorageLinkedService\",\n",
    "            type=\"LinkedServiceReference\",\n",
    "        ),\n",
    "         folderPath=storage_container_name,\n",
    "        fileName=storage_file_name,\n",
    "        compression= DatasetCompression(type=\"none\"),\n",
    "        location_type=\"AzureBlobStorageLocation\",\n",
    "        columnDelimiter = \",\",\n",
    "        rowDelimiter=\"\\n\",\n",
    "        first_row_as_header = True\n",
    "    ))\n",
    "print(\"Creating CSV dataset...\")\n",
    "ds_csv = df_client.datasets.create_or_update(resource_group, factory_name, \"AzureBlobCSVDataset\", storage_dataset)\n",
    "print(f\"csv dataset created {ds_csv.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Create Azure PostgreSQL Table Dataset\n",
    "postgres_dataset = DatasetResource(\n",
    "    properties=PostgreSqlTableDataset(\n",
    "        linked_service_name=LinkedService(\n",
    "            reference_name=\"AzurePostgreSqlLinkedService\",\n",
    "            type=\"LinkedServiceReference\"\n",
    "        ),\n",
    "       table_name = table_name\n",
    "    ))\n",
    "print(\"Creating postgres dataset...\")\n",
    "ds_postgres = df_client.datasets.create_or_update(resource_group, factory_name, \"AzurePostgreSqlDataset\", postgres_dataset)\n",
    "print(f\"postgres dataset created {ds_postgres.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.mgmt.datafactory.models import CopyActivity, PipelineResource, DatasetReference, DelimitedTextSource, AzurePostgreSqlSink,BlobSource\n",
    "\n",
    "# 4. Create Copy Activity\n",
    "\n",
    "copy_activity = CopyActivity(\n",
    "    name=\"CopyDataFromBlobToPostgres\",\n",
    "    inputs=[DatasetReference(reference_name=\"AzureBlobCSVDataset\",type=\"DatasetReference\")],\n",
    "    outputs=[DatasetReference(reference_name=\"AzurePostgreSqlDataset\",type=\"DatasetReference\")],\n",
    "    source=DelimitedTextSource(),\n",
    "    sink=AzurePostgreSqlSink(write_batch_timeout=\"00:05:00\", write_batch_size=10000)\n",
    ")\n",
    "\n",
    "# Create a pipeline with the copy activity\n",
    "pipeline_name = \"copyPipeline\"\n",
    "params_for_pipeline = {}\n",
    "pipeline_obj = PipelineResource(activities=[copy_activity], parameters=params_for_pipeline)\n",
    "pipeline = df_client.pipelines.create_or_update(resource_group, factory_name, pipeline_name, pipeline_obj)\n",
    "print(f\"Pipeline created: {pipeline.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "# Replace with your details\n",
    "pipeline_name = \"copyPipeline\"\n",
    "\n",
    "token = !az account get-access-token --query accessToken --output tsv\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {token[0]}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "url = f\"https://management.azure.com/subscriptions/{subscription_id}/resourceGroups/{resource_group}/providers/Microsoft.DataFactory/factories/{factory_name}/pipelines/{pipeline_name}/createRun?api-version=2018-06-01\"\n",
    "resp = requests.post(url, headers=headers, json={})\n",
    "print(\"Run pipeline response:\", resp.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_runs = f\"https://management.azure.com/subscriptions/{subscription_id}/resourceGroups/{resource_group}/providers/Microsoft.DataFactory/factories/{factory_name}/queryPipelineRuns?api-version=2018-06-01\"\n",
    "payload = {\"lastUpdatedAfter\": \"2023-01-01T00:00:00Z\", \"lastUpdatedBefore\": \"2024-01-01T00:00:00Z\"}\n",
    "runs_resp = requests.post(url_runs, headers=headers, json=payload)\n",
    "print(\"Pipeline runs:\", runs_resp.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
